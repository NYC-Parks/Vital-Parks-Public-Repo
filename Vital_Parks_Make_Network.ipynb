{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vital Parks Script Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the part of the vital parks script that builds the travel network and stores it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO! \n",
    "Add Ferries. Add City Bike. Add park paths. Add driving?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the assigned walk speed in miles per hr\n",
    "walk_speed_miles_per_hr = 3.0\n",
    "\n",
    "# If taking transit we take the average transit travel times on weekdays by default. \n",
    "# If weekend_transit=True then we take the average transit travel times on weekends.\n",
    "weekend_transit = False\n",
    "\n",
    "# If taking transit we take the average transit travel times during these hours (24 hr clock)\n",
    "transit_start_hr = 7\n",
    "transit_end_hr = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString, Polygon, MultiPolygon\n",
    "from shapely.geometry import Polygon, Point\n",
    "import requests\n",
    "import zipfile\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE STREET AND TRANSIT NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare street network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull and modify LION data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in lion data. It can only be read in 4000 row batches so we pull the entire dataset with a while loop\n",
    "not_finished = True\n",
    "max_object_id = 0\n",
    "data = {}\n",
    "i=0\n",
    "url = ('''https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/LION/FeatureServer/0/query''')\n",
    "while not_finished:\n",
    "    params = {\n",
    "        'where': '''(FeatureTyp IN ('0','6','A','C','W'))AND(NodeLevelF <> '*')AND(NodeLevelT <> '*')AND(NonPed <> 'V')AND(RW_TYPE <>' 2')AND(RW_TYPE <>'2')AND(RW_TYPE <>'11')AND(RW_TYPE <>'12')AND(RW_TYPE <>'13')AND(RW_TYPE <>'14')AND(OBJECTID>{})'''.format(max_object_id),\n",
    "        'outfields': '''OBJECTID,FeatureTyp,Street,SegmentID,RW_TYPE,NYPDID,FromLeft,ToLeft,FromRight,ToRight,XFrom,YFrom,XTo,YTo,NodeIDFrom,NodeIDTo,NodeLevelF,NodeLevelT,LBoro,RBoro,L_CD,R_CD,LCT2020,RCT2020,LCT2020Suf,RCT2020Suf,LCB2020,RCB2020,LCB2020Suf,RCB2020Suf''',\n",
    "        'outSR':'4326',\n",
    "        'limit':'4000',\n",
    "        'f': 'json'\n",
    "            }\n",
    "    response = requests.get(url,params=params)\n",
    "    data_current = response.json()\n",
    "    data[i] = pd.json_normalize(data_current['features'])\n",
    "    max_object_id = data_current['features'][-1]['attributes']['OBJECTID']\n",
    "    if len(data[i])<4000:\n",
    "        not_finished = False\n",
    "    i=i+1\n",
    "lion = pd.concat(data).reset_index(drop=True)\n",
    "lion.columns = lion.columns.str.lstrip('attributes.')\n",
    "lion = lion.rename(columns={'geometry.paths':'geometry'})\n",
    "lion['geometry'] = lion['geometry'].apply(lambda x: LineString(x[0]))\n",
    "lion = gpd.GeoDataFrame(lion,geometry='geometry',crs=4326).to_crs(2263)\n",
    "\n",
    "#Make sure source and target nodes include node-level as part of its unique ID so we keep track of how roads actually connect accounting for both location and level\n",
    "lion['source']=lion['NodeIDFrom']+lion['NodeLevelF']\n",
    "lion['target']=lion['NodeIDTo']+lion['NodeLevelT']\n",
    "\n",
    "#Change names of From/To so it matches our Source/Target vocabulary as we keep track of lat/lon coordinates for each node\n",
    "lion = lion.rename(columns={'XFrom':'XCoord_source','YFrom':'YCoord_source','XTo':'XCoord_target','YTo':'YCoord_target'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert walk speed to feet since we will be using crs=2263 for this script which measures distance in feet\n",
    "walk_speed_feet_per_second = walk_speed_miles_per_hr*1.46667\n",
    "\n",
    "# Group by segment ID so segmentID becomes a unique ID.\n",
    "# This is needed because there are times when multiple rows have the same SegmentID but in these cases they are actually representing the same physical segment \n",
    "# These duplications exist for to other Lion dataset uses that we do not need for this instance.\n",
    "lion_walkable = lion.groupby('SegmentID').agg({'source':'min','target':'max','Street':'first','FeatureTyp':'min',\n",
    "                                                                           'RW_TYPE':'min','FromLeft':'min','ToLeft':'max','FromRight':'min','ToRight':'max',\n",
    "                                                                           'NYPDID':'first','LBoro':'first','RBoro':'first','L_CD':'first','R_CD':'first',\n",
    "                                                                           'LCT2020':'first','RCT2020':'first','LCT2020Suf':'first','RCT2020Suf':'first',\n",
    "                                                                           'LCB2020':'first', 'RCB2020':'first', 'LCB2020Suf':'first', 'RCB2020Suf':'first',\n",
    "                                                                           'XCoord_source':'mean','YCoord_source':'mean','XCoord_target':'mean','YCoord_target':'mean',\n",
    "                                                                           'geometry':'first'}).reset_index()\n",
    "lion_walkable = gpd.GeoDataFrame(lion_walkable,geometry='geometry',crs=2263)\n",
    "\n",
    "# Assign each segment a weight equal to the number of seconds required to travel (walk) each segment\n",
    "lion_walkable['length'] = lion_walkable['geometry'].apply(lambda x: float(x.length))\n",
    "lion_walkable['weight'] = lion_walkable['length']/walk_speed_feet_per_second\n",
    "lion_walkable['mode'] = 'walk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up each street centerline into its left/right block.\n",
    "# Assign each block a unique idea by its segmentID + (L or R)\n",
    "\n",
    "# Get left blocks from street centerline data\n",
    "lion_walkable_left = lion_walkable[['SegmentID','Street','source','target','weight','mode','FeatureTyp','RW_TYPE','FromLeft','ToLeft','length',\n",
    "                                    'NYPDID','LBoro','L_CD','LCT2020','LCT2020Suf','LCB2020','LCB2020Suf',\n",
    "                                    'XCoord_source','YCoord_source','XCoord_target','YCoord_target','geometry']]\n",
    "lion_walkable_left = lion_walkable_left.rename(columns={'FromLeft':'From','ToLeft':'To','LBoro':'Boro','L_CD':'CommunityBoard',\n",
    "                                                        'LCT2020':'CT2020','LCT2020Suf':'CT2020Suf','LCB2020':'CB2020','LCB2020Suf':'CB2020Suf'})\n",
    "lion_walkable_left['SideOfStreet'] = 'L'\n",
    "lion_walkable_left['uniqueID'] = lion_walkable_left['SegmentID']+'L'\n",
    "\n",
    "# Get right blocks from street centerline data\n",
    "# We flip the order of source/target nodes for the right blocks. This is just a way to make sure each street segment in our network can be walked in both directions.\n",
    "lion_walkable_right = lion_walkable[['SegmentID','Street','source','target','weight','mode','FeatureTyp','RW_TYPE','FromRight','ToRight','length',\n",
    "                                    'NYPDID','RBoro','R_CD','RCT2020','RCT2020Suf','RCB2020','RCB2020Suf',\n",
    "                                    'XCoord_source','YCoord_source','XCoord_target','YCoord_target','geometry']]\n",
    "lion_walkable_right = lion_walkable_right.rename(columns={'source':'target','target':'source','FromRight':'From','ToRight':'To','RBoro':'Boro','R_CD':'CommunityBoard',\n",
    "                                                        'RCT2020':'CT2020','RCT2020Suf':'CT2020Suf','RCB2020':'CB2020','RCB2020Suf':'CB2020Suf',\n",
    "                                                        'XCoord_source':'XCoord_target','YCoord_source':'YCoord_target',\n",
    "                                                        'XCoord_target':'XCoord_source','YCoord_target':'YCoord_source'})\n",
    "lion_walkable_right['SideOfStreet'] = 'R'\n",
    "lion_walkable_right['uniqueID'] = lion_walkable_right['SegmentID']+'R'\n",
    "\n",
    "# Concat left and right blocks together to get all blocks throughout the city included in our walkable network\n",
    "lion_walkable = pd.concat([lion_walkable_left,lion_walkable_right])\n",
    "\n",
    "# For each source-target pair we make an edge label in the format of networkX as we will compare these edges to network edges soon.\n",
    "lion_walkable['edges'] = list(zip(lion_walkable['source'],lion_walkable['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect LION walkable streets to OSM park paths (To Do in future version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import lion data into networkx data structure so we can remove small disconnected areas that are not part of this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a network graph in networkX from our lion_walkable dataframe\n",
    "Walk_DG = nx.from_pandas_edgelist(lion_walkable, create_using=nx.DiGraph())\n",
    "\n",
    "# Remove all but largest two connected components (Walkably connected NYC + Walkably connected Staten Island)\n",
    "Walk_DG = Walk_DG.subgraph([p for ps in sorted(list(nx.strongly_connected_components(Walk_DG)), key=len, reverse=True)[:2] for p in ps])\n",
    "\n",
    "# Restrict our lion_walkable table to only include edges in one of these two largest connected components\n",
    "lion_walkable = lion_walkable[lion_walkable['edges'].isin(list(Walk_DG.edges))]\n",
    "lion_walkable = lion_walkable.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Lion Data to Census Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are a few lion streets that have no census geography info. We keep these streets for potential walking uses but will assign them no population.\n",
    "lion_walkable.loc[lion_walkable['Boro'].isna(),'Boro'] = 0\n",
    "\n",
    "# This function converts the hierarchical census codes of county (borough), census-tract, and census-block to a single census 'geo_ID'\n",
    "def create_census_GeoID(lion_in):\n",
    "    Boro_Code = lion_in['Boro'].astype(int).astype(str)\n",
    "    Boro_Code.loc[Boro_Code=='1'] = '061'\n",
    "    Boro_Code.loc[Boro_Code=='2'] = '005'\n",
    "    Boro_Code.loc[Boro_Code=='3'] = '047'\n",
    "    Boro_Code.loc[Boro_Code=='4'] = '081'\n",
    "    Boro_Code.loc[Boro_Code=='5'] = '085'\n",
    "    Tract = lion_in['CT2020'].str.replace(' ','0')\n",
    "    Tract_Suf = lion_in['CT2020Suf'].str.replace(' ','0')\n",
    "    Block_Code = lion_in['CB2020'].str[0]\n",
    "    \n",
    "    geo_ID = Boro_Code + Tract + Tract_Suf + Block_Code\n",
    "    \n",
    "    return geo_ID\n",
    "\n",
    "# Create GEO_ID from each blocks geographical data so that we merge census data to city blocks\n",
    "lion_walkable['Geo_ID'] = create_census_GeoID(lion_walkable)\n",
    "\n",
    "\n",
    "# Assign 0 'livable length' to blocks that have no addresses on them. We will then proportionally assign census block populations according to each city block's livable length.\n",
    "lion_walkable['livable_length'] = lion_walkable['length']\n",
    "lion_walkable.loc[((lion_walkable['From']==0)&(lion_walkable['To']==0)),'livable_length'] = 0\n",
    "lion_walkable.loc[lion_walkable['Geo_ID'].isna(),'livable_length'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and format 2020 census population data to join to lion dataset\n",
    "url = ('''https://api.census.gov/data/2020/dec/dhc?get=group(P1)&for=block%20group:*&in=state:36&in=county:061,005,047,081,085&in=tract:*''')\n",
    "response = requests.get(url)\n",
    "Census_data = response.json()\n",
    "Census_data = pd.DataFrame(Census_data[1:],columns=Census_data[0])\n",
    "Census_data['GEO_ID'] = Census_data['GEO_ID'].str[-10:]\n",
    "Census_data = Census_data[['GEO_ID','P1_001N']].rename(columns={'GEO_ID':'Geo_ID','P1_001N':'BG_total_pop'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign census-block population data to city blocks proportional to each city-blocks livable length\n",
    "\n",
    "# Calculate the proportion of livable length that each city-block makes up for in its respective census-block-group\n",
    "total_liveable_lengths = lion_walkable[['Geo_ID','livable_length']].groupby('Geo_ID').sum().reset_index().rename(columns={'livable_length':'Total_geo_ID_livable_length'})\n",
    "lion_walkable = lion_walkable.merge(total_liveable_lengths, how='left', on='Geo_ID')\n",
    "lion_walkable.loc[:,'Proportion_of_blockgroup'] = lion_walkable['livable_length']/(lion_walkable['Total_geo_ID_livable_length']+.00001)\n",
    "\n",
    "# Merge with census data and assign population from each census block group to each city block according to the above calculated proportion\n",
    "lion_walkable = lion_walkable.merge(Census_data, how='left', on='Geo_ID')\n",
    "lion_walkable.loc[lion_walkable['BG_total_pop'].isna(),'BG_total_pop'] = 0\n",
    "lion_walkable['BG_total_pop'] = lion_walkable['BG_total_pop'].astype(float)\n",
    "lion_walkable['population'] = lion_walkable['Proportion_of_blockgroup']*lion_walkable['BG_total_pop']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Lion streets to the geographical regions of interest that are not already in lion features data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign lion streets to City Council Districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and formate council district boundaries\n",
    "\n",
    "url = ('''https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_City_Council_Districts/FeatureServer/0/query?where=1=1&outFields=*&outSR=4326&f=pgeojson''')\n",
    "council_district_shapes = gpd.read_file(url,params=params)\n",
    "council_district_shapes = council_district_shapes[['CounDist','geometry']].to_crs(2263)\n",
    "council_district_shapes = council_district_shapes.rename(columns={'CounDist':'Council_District'})\n",
    "council_district_shapes['Council_District'] = council_district_shapes['Council_District'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign city blocks to council districts based on geographic data. (This is an estimate and could be made more precise with some geographical research.)\n",
    "\n",
    "council_near = lion_walkable.loc[:,['uniqueID','geometry']].sjoin_nearest(council_district_shapes, how='left',distance_col='dis')\n",
    "council_near = council_near.sort_values('Council_District',ascending=False).sort_values('dis',ascending=False)\n",
    "council_near = council_near.groupby('uniqueID').first().reset_index()\n",
    "council_near = council_near[['uniqueID','Council_District']]\n",
    "\n",
    "lion_walkable = lion_walkable.merge(council_near,on='uniqueID',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign lion streets to yes/no state designated disadvantaged communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and formate disadvantaged community census tracts\n",
    "\n",
    "url = ('''https://data.ny.gov/resource/2e6c-s6fp.geojson?$limit=10000&$where=nyc_region='NYC' ''')\n",
    "DAC_shapes = gpd.read_file(url)\n",
    "\n",
    "DAC_shapes = DAC_shapes[['geoid','dac_designation','geometry']].to_crs(2263)\n",
    "DAC_shapes['geoid'] = DAC_shapes['geoid'].str[2:]\n",
    "DAC_shapes = DAC_shapes.rename(columns={'geoid':'DAC_ID','dac_designation':'DAC_Designation'})\n",
    "\n",
    "DAC_shapes = DAC_shapes.dissolve(by='DAC_ID').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First try to match on ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match most blocks to this set of DAC tracts based on their Geo_ID.\n",
    "\n",
    "lion_walkable['DAC_ID'] = lion_walkable['Geo_ID'].str[:-1]\n",
    "lion_walkable = lion_walkable.merge(DAC_shapes[['DAC_ID','DAC_Designation']],how='left',on='DAC_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Match remaining unaccounted for streets with geo-spacial join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most DAC tracts match to our Geo_IDs but a few of them dont match perfectly probably for some census tract details I cant perfectly answer.\n",
    "# Due to this small issue with matching DAC_IDs to Geo_IDs we match the remaining blocks to their nearest DAC/NotDAC census tract based on geography.\n",
    "# Note that this step is also an approximation and could be improved with some geographical research\n",
    "\n",
    "DAC_near = lion_walkable[lion_walkable['DAC_Designation'].isna()].loc[:,['uniqueID','geometry']].sjoin_nearest(DAC_shapes, how='left',distance_col='dis')\n",
    "DAC_near = DAC_near.sort_values('DAC_Designation',ascending=False).sort_values('dis',ascending=False)\n",
    "DAC_near = DAC_near.groupby('uniqueID').first().reset_index()\n",
    "DAC_near = DAC_near[['uniqueID','DAC_ID','DAC_Designation']].rename(columns={'DAC_ID':'DAC_ID_geom','DAC_Designation':'DAC_Designation_geom'})\n",
    "\n",
    "lion_walkable = lion_walkable.merge(DAC_near,on='uniqueID',how='left')\n",
    "lion_walkable.loc[lion_walkable['DAC_Designation'].isna(),'DAC_ID'] = lion_walkable.loc[lion_walkable['DAC_Designation'].isna(),'DAC_ID_geom']\n",
    "lion_walkable.loc[lion_walkable['DAC_Designation'].isna(),'DAC_Designation'] = lion_walkable.loc[lion_walkable['DAC_Designation'].isna(),'DAC_Designation_geom']\n",
    "lion_walkable = lion_walkable.drop(columns=['DAC_ID_geom','DAC_Designation_geom'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign lion streets to Gun Violence Prevention police precincts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and format Police Precinct boundaries\n",
    "\n",
    "url = ('''https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Police_Precincts/FeatureServer/0/query?where=1%3D1&objectIds=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&resultType=none&distance=0.0&units=esriSRUnit_Meter&relationParam=&returnGeodetic=false&outFields=&returnGeometry=true&returnCentroid=false&returnEnvelope=false&featureEncoding=esriDefault&multipatchOption=xyFootprint&maxAllowableOffset=&geometryPrecision=&outSR=&defaultSR=&datumTransformation=&applyVCSProjection=false&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnExtentOnly=false&returnQueryGeometry=false&returnDistinctValues=false&cacheHint=false&collation=&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&returnZ=false&returnM=false&returnTrueCurves=false&returnExceededLimitFeatures=true&quantizationParameters=&sqlFormat=none&f=pgeojson&token=''')\n",
    "\n",
    "precinct_shapes = gpd.read_file(url)\n",
    "precinct_shapes = precinct_shapes[['Precinct','geometry']].to_crs(2263)\n",
    "precinct_shapes.loc[precinct_shapes['Precinct'].isin([40,42,44,47,73,75]),'GVP'] = 'Yes GVP'\n",
    "precinct_shapes.loc[~precinct_shapes['Precinct'].isin([40,42,44,47,73,75]),'GVP'] = 'No GVP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign blocks to their police precinct by geography. This is an approximation that could be improved with geographical research.\n",
    "\n",
    "precinct_near = lion_walkable.loc[:,['uniqueID','geometry']].sjoin_nearest(precinct_shapes, how='left',distance_col='dis')\n",
    "precinct_near = precinct_near.sort_values('Precinct',ascending=False).sort_values('dis',ascending=False)\n",
    "precinct_near = precinct_near.groupby('uniqueID').first().reset_index()\n",
    "precinct_near = precinct_near[['uniqueID','Precinct','GVP']]\n",
    "\n",
    "lion_walkable = lion_walkable.merge(precinct_near,on='uniqueID',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Lion Data to TRIE communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and format TRIE community boundaries.\n",
    "\n",
    "url = ('''https://services3.arcgis.com/xJHn8F2NTtwCMFtX/ArcGIS/rest/services/TRIE/FeatureServer/0/query''')\n",
    "\n",
    "params = {\n",
    "    'where': '1=1',\n",
    "    'outfields': '*',\n",
    "    'f': 'json'\n",
    "}\n",
    "response = requests.get(url,params=params)\n",
    "Trie_shapes = response.json()\n",
    "Trie_shapes = pd.json_normalize(Trie_shapes['features'])\n",
    "for row in Trie_shapes.index:\n",
    "    Trie_shapes.loc[row,'geometry'] = Polygon(Trie_shapes.loc[row,'geometry.rings'][0])  \n",
    "Trie_shapes = Trie_shapes[['attributes.OBJECTID','attributes.Neighborhood','geometry']].rename(columns={'attributes.OBJECTID':'Trie_label','attributes.Neighborhood':'Trie_name'})\n",
    "Trie_shapes = gpd.GeoDataFrame(Trie_shapes,geometry='geometry',crs=2263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign city blocks to TRIE communities based on geographical approximations. Perhaps can be improved with more geographical research.\n",
    "\n",
    "trie_intersections = gpd.overlay(lion_walkable[['geometry','uniqueID','length']],Trie_shapes)\n",
    "trie_intersections['len'] = trie_intersections.length\n",
    "trie_intersections = trie_intersections.sort_values('len', ascending=False)\n",
    "trie_intersections = trie_intersections.groupby('uniqueID').first().reset_index()\n",
    "trie_intersections.loc[trie_intersections['len']<(.3*trie_intersections['length']),'Trie_label'] = 0\n",
    "trie_intersections.loc[trie_intersections['len']<(.3*trie_intersections['length']),'Trie_name'] = 'Not Trie'\n",
    "trie_intersections = trie_intersections[['uniqueID','Trie_label','Trie_name']]\n",
    "\n",
    "lion_walkable = lion_walkable.merge(trie_intersections,on='uniqueID',how='left')\n",
    "lion_walkable.loc[lion_walkable['Trie_label'].isna(),'Trie_label']=0\n",
    "lion_walkable.loc[lion_walkable['Trie_name'].isna(),'Trie_name']='Not Trie'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Walkable_DG graph with full lion_walkable atributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NetworkX graph based on these city-block edges and store each blocks geographical and population data as edge attributes\n",
    "\n",
    "Walk_DG = nx.from_pandas_edgelist(lion_walkable, create_using=nx.DiGraph(), \n",
    "                             edge_attr=['uniqueID','Street', 'length','geometry', 'SegmentID','XCoord_source','YCoord_source','XCoord_target','YCoord_target',\n",
    "                                        'RW_TYPE','FeatureTyp','NYPDID','SideOfStreet','From','To','Boro', 'CommunityBoard','weight', 'mode',\n",
    "                                        'CT2020','CT2020Suf','CB2020','CB2020Suf', 'Geo_ID', 'livable_length','Total_geo_ID_livable_length','Proportion_of_blockgroup',\n",
    "                                        'BG_total_pop', 'population', 'DAC_ID','DAC_Designation', 'Precinct', 'GVP','Trie_label','Trie_name','Council_District'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also store geographic location information (lat/lon) for all of our nodes (segment connections)\n",
    "\n",
    "lion_walkable_nodes = pd.concat([lion_walkable[['source','XCoord_source','YCoord_source']].rename(columns={'XCoord_source':'x','YCoord_source':'y', 'source':'NodeID'}),\n",
    "                                    lion_walkable[['target','XCoord_target','YCoord_target']].rename(columns={'XCoord_target':'x','YCoord_target':'y', 'target':'NodeID'})])\n",
    "lion_walkable_nodes = lion_walkable_nodes.drop_duplicates(subset='NodeID').set_index('NodeID').apply(lambda x: Point(x['x'],x['y']), axis=1)\n",
    "\n",
    "nx.set_node_attributes(Walk_DG, lion_walkable_nodes.to_dict(), name='geometry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Transit network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import stops and stoptime gtfs data for MTA subways and buses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in regular schedule MTA data (This data gets updated every 3 months-ish)\n",
    "\n",
    "def gtfs_mta_import(url):\n",
    "    response = requests.get(url)\n",
    "    file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "    stops_out = pd.read_csv(file.open('stops.txt'))[['stop_id', 'stop_name', 'stop_lat', 'stop_lon']]\n",
    "    stops_out['stop_id'] = stops_out['stop_id'].astype(str)\n",
    "    stop_times_out = pd.read_csv(file.open('stop_times.txt'))[['trip_id', 'stop_id', 'arrival_time', 'departure_time','stop_sequence']]\n",
    "    stop_times_out['stop_id'] = stop_times_out['stop_id'].astype(str)\n",
    "    return stops_out, stop_times_out\n",
    "\n",
    "# We include city bus, special bus, and subway data. LIRR and other transit data can also be found on the MTA developer page if one is interested\n",
    "bx_bus_stops, bx_bus_stop_times = gtfs_mta_import('''https://rrgtfsfeeds.s3.amazonaws.com/gtfs_bx.zip''')\n",
    "bk_bus_stops, bk_bus_stop_times = gtfs_mta_import('''https://rrgtfsfeeds.s3.amazonaws.com/gtfs_b.zip''')\n",
    "mn_bus_stops, mn_bus_stop_times = gtfs_mta_import('''https://rrgtfsfeeds.s3.amazonaws.com/gtfs_m.zip''')\n",
    "qn_bus_stops, qn_bus_stop_times = gtfs_mta_import('''https://rrgtfsfeeds.s3.amazonaws.com/gtfs_q.zip''')\n",
    "si_bus_stops, si_bus_stop_times = gtfs_mta_import('''https://rrgtfsfeeds.s3.amazonaws.com/gtfs_si.zip''')\n",
    "exp_bus_stops, exp_bus_stop_times = gtfs_mta_import('''https://rrgtfsfeeds.s3.amazonaws.com/gtfs_busco.zip''')\n",
    "subway_stops, subway_stop_times = gtfs_mta_import('''https://rrgtfsfeeds.s3.amazonaws.com/gtfs_subway.zip''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format stop locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = ('''https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Borough_Boundary/FeatureServer/0/query?where=1=1&outFields=*&outSR=4326&f=pgeojson''')\n",
    "response = requests.get(url)\n",
    "boros = gpd.GeoDataFrame.from_features(response.json()).set_crs(4326).to_crs(2263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format stops data\n",
    "def format_mta_stops_data(stops_in):\n",
    "    stops_in['geometry'] = stops_in.apply(lambda x: Point(x['stop_lon'], x['stop_lat']), axis=1)\n",
    "    stops_in = gpd.GeoDataFrame(stops_in, geometry='geometry', crs='epsg:4326').to_crs('epsg:2263')\n",
    "    stops_in['stop_id'] = stops_in['stop_id'].apply(str)\n",
    "    return stops_in\n",
    "\n",
    "stops = pd.concat([bx_bus_stops, bk_bus_stops, mn_bus_stops, qn_bus_stops, si_bus_stops, exp_bus_stops, subway_stops])\n",
    "stops = format_mta_stops_data(stops)\n",
    "\n",
    "# Only include stops within the city limits\n",
    "city = gpd.GeoDataFrame([boros.unary_union.buffer(100)]).rename(columns={0:'geometry'}).set_geometry(col='geometry', crs=2263)\n",
    "stops = gpd.sjoin(stops,city, how='left')\n",
    "stops = stops.loc[~stops['index_right'].isna(),['stop_id','stop_name','geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add edges that connect each stop to its nearest level M (ground level) street node.  \n",
    "# Add a weight for how long it takes to walk from street node to stop node or vice versa.\n",
    "\n",
    "lion_street_nodes_gdf = gpd.GeoDataFrame(lion_walkable_nodes).reset_index().rename(columns={0:'geometry'}).set_geometry(col='geometry',crs=2263)\n",
    "lion_street_nodes_gdf = lion_street_nodes_gdf[lion_street_nodes_gdf['NodeID'].str[-1]=='M']\n",
    "stop_node_connections = gpd.sjoin_nearest(stops,lion_street_nodes_gdf, how='left', distance_col='dist')\n",
    "stop_node_connections = stop_node_connections.sort_values('dist')\n",
    "stop_node_connections = stop_node_connections.groupby(['stop_id','NodeID']).agg({'stop_name':'first','dist':'min'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we can both enter and exit every stop by swapping the source and target and concatenating the resulting dataframes\n",
    "\n",
    "stop_node_connections['stop_id'] = stop_node_connections['stop_id']+'_mta'\n",
    "stop_node_connections['mode'] = 'walk_mta_connection'\n",
    "stop_node_connections['weight'] = stop_node_connections['dist']/walk_speed_feet_per_second\n",
    "stop_node_connections_exit = stop_node_connections.rename(columns={'stop_id':'source','NodeID':'target'})\n",
    "stop_node_connections_enter = stop_node_connections.rename(columns={'stop_id':'target','NodeID':'source'})\n",
    "stop_node_connections = pd.concat([stop_node_connections_enter,stop_node_connections_exit])\n",
    "\n",
    "# Create a networkX network made of of edges that connect each MTA stop to its nearest street level node\n",
    "walk_to_MTA_DG = nx.from_pandas_edgelist(stop_node_connections, create_using=nx.DiGraph(), \n",
    "                             edge_attr=['stop_name','weight','mode'])\n",
    "\n",
    "# Stitch the walkable street network and the enter/exit MTA stop locations network together\n",
    "complete_DG = nx.compose(Walk_DG,walk_to_MTA_DG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate travel times (including expected wait times) between connected stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code chunk calculates the average travel time between any connected pair of MTA stops.\n",
    "# This average is with respect to the times specified by our initial parameters: \n",
    "##  weekend_transit (True/False), transit_start_hr (INT btwn 0-24), transit_end_hr (INT btwn transit_start_hr-24)\n",
    "### (Intuitively, we are calculating the average (fastest possible travel time between each feasible stop-pair\n",
    "###  where this average is over all possible trip start times that fall between our set transit_start_hr and transit_end_hr parameters.)\n",
    "\n",
    "# Format stop-times data\n",
    "stop_times = pd.concat([bx_bus_stop_times, bk_bus_stop_times, mn_bus_stop_times, qn_bus_stop_times, si_bus_stop_times, exp_bus_stop_times, subway_stop_times])\n",
    "stop_times['stop_id'] = stop_times['stop_id'].astype(str)\n",
    "stop_times['stop_id'] = stop_times['stop_id']+'_mta'\n",
    "\n",
    "# Remove any stops that are not part of our walkable-street to MTA stop connected network. (e.g. not in city boundaries like Hoboken...)\n",
    "stop_times = stop_times[stop_times['stop_id'].isin(stop_node_connections_exit['source'].unique())]\n",
    "\n",
    "# If weekend_transit is set to false then only include weekday trips\n",
    "if weekend_transit==False:\n",
    "    trips = stop_times[stop_times['trip_id'].str.contains('Weekday')]\n",
    "\n",
    "# Only consider trips between our chosen start and end times\n",
    "trips = trips.loc[(trips['departure_time']>=str(transit_start_hr).zfill(2))&(trips['departure_time']<=str(transit_end_hr).zfill(2))]\n",
    "\n",
    "# Create the collection of pairwise stop-to-stop connections. \n",
    "## (We take this more detailed approach of stop-to-stop edges instead of the previous route based approach because we want to account for the 14th to w4 situation.\n",
    "##  In this situation you would happily take either the A, C, or E train, which ever comes first.\n",
    "##  So when you arrive a at 14th street you are not waiting for any MTA route one but for the first of three possible MTA routes that arrives.)\n",
    "\n",
    "# Connect any two stops that are part of a shared route.\n",
    "departures = trips[['trip_id','departure_time','stop_id','stop_sequence']].rename(columns={'stop_id':'departure_stop','stop_sequence':'dep_sequence'})\n",
    "arrivals = trips[['trip_id','arrival_time','stop_id','stop_sequence']].rename(columns={'stop_id':'arrival_stop','stop_sequence':'arr_sequence'})\n",
    "stop_to_stop = departures.merge(arrivals, on='trip_id',how='left')\n",
    "\n",
    "# Stop connections are directed so only include the situations where the departure stop comes before the arrival stop in that route\n",
    "stop_to_stop = stop_to_stop[stop_to_stop['dep_sequence']<stop_to_stop['arr_sequence']]\n",
    "\n",
    "# Provide a unique label for each feasible departure_stop-to-arrival_stop pairing\n",
    "stop_to_stop['stop_pair']=stop_to_stop['departure_stop'].astype(str)+'_'+stop_to_stop['arrival_stop'].astype(str)\n",
    "stop_to_stop = stop_to_stop[['trip_id', 'departure_time', 'departure_stop','arrival_time', 'arrival_stop', 'stop_pair']].reset_index(drop=True)\n",
    "\n",
    "# There are some stop to stop connections one should never take such as riding a local train to get between far apart express stops. \n",
    "# We remove these artificially slower connections before proceeding to calculate the average stop-to-stop travel times.\n",
    "dep_arr_ranks = stop_to_stop.groupby('stop_pair')[['departure_time','arrival_time']].rank('min').rename(columns={'departure_time':'sts_depart_rank','arrival_time':'sts_arrive_rank'})\n",
    "stop_to_stop['sts_depart_rank'] = dep_arr_ranks['sts_depart_rank']\n",
    "stop_to_stop['sts_arrive_rank'] = dep_arr_ranks['sts_arrive_rank']\n",
    "stop_to_stop = stop_to_stop.sort_values('sts_arrive_rank').reset_index(drop=True)\n",
    "drop_trips = stop_to_stop.groupby('stop_pair')['sts_depart_rank'].diff()\n",
    "while ((drop_trips<=0).sum()>0):\n",
    "    stop_to_stop = stop_to_stop[~(drop_trips<=0)]\n",
    "    drop_trips = stop_to_stop.groupby('stop_pair')['sts_depart_rank'].diff()\n",
    "\n",
    "# Here we calculate the expected wait time for each possible stop-to-stop trip. \n",
    "# This is equal to half the time between the last feasible departure and the next feasible departure for each stop-to-stop pair.\n",
    "stop_to_stop = stop_to_stop.reset_index(drop=True)\n",
    "stop_to_stop = stop_to_stop[['departure_time', 'departure_stop', 'arrival_time','arrival_stop', 'stop_pair', 'sts_depart_rank']]\n",
    "stop_to_stop['sts_depart_rank'] = stop_to_stop.groupby('stop_pair')['departure_time'].rank()\n",
    "previous_depart = stop_to_stop[['stop_pair','sts_depart_rank','departure_time']].rename(columns={'departure_time':'prev_departure_time'})\n",
    "previous_depart['sts_depart_rank'] = previous_depart['sts_depart_rank']+1\n",
    "stop_to_stop = stop_to_stop.merge(previous_depart, on=['stop_pair','sts_depart_rank'], how='left')\n",
    "stop_to_stop.loc[stop_to_stop['prev_departure_time'].isna(),'prev_departure_time'] = str(transit_start_hr).zfill(2)+':00:00'\n",
    "stop_to_stop['departure_time'] = pd.to_datetime(stop_to_stop['departure_time'],format='%H:%M:%S')\n",
    "stop_to_stop['arrival_time'] = pd.to_datetime(stop_to_stop['arrival_time'],format='%H:%M:%S')\n",
    "stop_to_stop['prev_departure_time'] = pd.to_datetime(stop_to_stop['prev_departure_time'],format='%H:%M:%S')\n",
    "stop_to_stop['wait_time_secs'] = (stop_to_stop['departure_time']-stop_to_stop['prev_departure_time']).dt.total_seconds()/2\n",
    "\n",
    "# Here we calculate the actual in transit time for each trip\n",
    "stop_to_stop['travel_time_secs'] = (stop_to_stop['arrival_time']-stop_to_stop['departure_time']).dt.total_seconds()\n",
    "\n",
    "# Here we add the expected wait time and the actual transit time for each trip to get the total travel time for each trip\n",
    "stop_to_stop['total_transit_time_secs'] = stop_to_stop['wait_time_secs']+stop_to_stop['travel_time_secs']\n",
    "\n",
    "# Now that we have calculated the total travel time for each possible stop-to-stop trip \n",
    "# we compute the average travel time for every feasible stop-to-stop pair during our parameter defined travel window\n",
    "stop_to_stop = stop_to_stop.groupby('stop_pair').agg({'departure_stop':'first','arrival_stop':'first','total_transit_time_secs':'median'}).reset_index()\n",
    "\n",
    "# We format these transit connections to match the source/target convention used by networkX\n",
    "stop_to_stop['mode'] = 'mta'\n",
    "stop_to_stop = stop_to_stop[['departure_stop','arrival_stop','mode','total_transit_time_secs']].rename(columns={'departure_stop':'source','arrival_stop':'target','total_transit_time_secs':'weight'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the directed network of stop-to-stop mta connections where the weight of each edge is the average travel time (in seconds) for that stop-to-stop pair\n",
    "\n",
    "MTA_DG = nx.from_pandas_edgelist(stop_to_stop, create_using=nx.DiGraph(), edge_attr=['weight','mode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We stitch this directed transit network together with our other network of both walkable streets and (MTA stops)-to-(walkable streets) connections\n",
    "\n",
    "complete_DG = nx.compose(complete_DG,MTA_DG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have now finished creating our networkX walking and MTA NYC network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below we choose to store the parts of this network we want to keep as geodataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we store the entire network edgeset in one geodataframe which we will use in future vital parks scripts\n",
    "\n",
    "Vital_Parks_Edges = nx.to_pandas_edgelist(complete_DG)\n",
    "Vital_Parks_Edges = gpd.GeoDataFrame(Vital_Parks_Edges, geometry='geometry').set_crs(2263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we store the street-level nodes in a geodataframe because we want to keep the geo-location data for these particular nodes for the remaining Vital Parks Scripts\n",
    "\n",
    "Vital_Parks_Street_Nodes = pd.DataFrame([nx.get_node_attributes(complete_DG, 'geometry')]).transpose().reset_index().rename(columns={0:'geometry','index':'street_node_id'})\n",
    "Vital_Parks_Street_Nodes = gpd.GeoDataFrame(Vital_Parks_Street_Nodes, geometry='geometry').set_crs(2263)\n",
    "\n",
    "# Restrict this set of street points so that we can only access the network at ground level (Level 'M').\n",
    "# This prevents us, for example, from 'entering' the street network in the middle of a bridge. We would only include the entrance nodes for this bridge and none of the higher midpoints.\n",
    "Vital_Parks_Street_Nodes = Vital_Parks_Street_Nodes[Vital_Parks_Street_Nodes['street_node_id'].str.contains('M')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################\n",
    "# Upload/save these edges and nodes geodataframes to SQL server, as a GeoJSON, or however you want to store it so that they can be accessed by the third vital parks script\n",
    "# ##################################################\n",
    "\n",
    "# Vital_Parks_Edges.to_file('Network_edges.geojson', driver='GeoJSON')\n",
    "\n",
    "# Vital_Parks_Street_Nodes.to_file('Network_nodes.geojson', driver='GeoJSON') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END Vital Parks Script Part 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
